sudo apt-get install openjdk-8-jdk
java -version
sudo nano /etc/environment
JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
JRE_HOME="/usr/lib/jvm/java-8-openjdk-amd64/jre"
sudo adduser hadoop sudo
passwd hadoop
su - hadoop
sudo apt-get install ssh
Sudo aptâ€“get install pdsh
ssh-keygen -t rsa -b 4096
cat .ssh/id_rsa.pub >> .ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys
ssh localhost
wget http://apache.cs.utah.edu/hadoop/common/hadoop-3.1.1/hadoop-3.1.1.tar.gz
tar -xzf hadoop-3.1.1.tar.gz
sudo mv hadoop-3.1.1 /home/hadoop/hadoop
nano .bashrc
export HADOOP_HOME=/home/hadoop/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME 
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HIVE_HOME=/home/hadoop/hive
export PATH=$PATH:/home/hadoop/hive/bin
export SPARK_HOME=/home/hadoop/spark
export PATH=$SPARK_HOME/bin:$PATH
export PATH=$PATH:~/.local/bin
source .bashrc
nano /home/hadoop/hadoop/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
nano /home/hadoop/hadoop/etc/hadoop/core-site.xml
<configuration> 
    <property> 
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hadoop.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hadoop.groups</name>
        <value>*</value>
    </property>
</configuration>
nano /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.name.dir</name> <value>file:///home/hadoop/hadoopdata/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.data.dir</name> <value>file:///home/hadoop/hadoopdata/hdfs/datanode</value>
    </property>
</configuration>
nano  /home/hadoop/hadoop/etc/hadoop/mapred-site.xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>
    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>3072</value>
    </property>
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>3072</value>
    </property>
</configuration>
nano  /home/hadoop/hadoop/etc/hadoop/yarn-site.xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration>
hdfs namenode -format
export PDSH_RCMD_TYPE=ssh
start-dfs.sh
start-yarn.sh
hdfs dfsadmin -report
hadoop fs -mkdir /user
hadoop fs -mkdir /user/hadoop
wget https://www-eu.apache.org/dist/hive/hive-3.1.1/apache-hive-3.1.1-bin.tar.gz
tar -xzf apache-hive-3.1.1-bin.tar.gz
mv apache-hive-3.1.1-bin /home/hadoop/hive
hive --version
hadoop fs -mkdir -p /user/hive/warehouse
hadoop fs -chmod g+w /user/hive/warehouse
hadoop fs -mkdir -p /tmp
hadoop fs -chmod g+w /tmp
cp hive/conf/hive-env.sh.template hive/conf/hive-env.sh
nano hive/conf/hive-env.sh
export HADOOP_HOME=/home/hadoop/hadoop
export HIVE_CONF_DIR=/home/hadoop/hive/conf
nano hive/conf/hive-site.xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?><!--Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. --> 

<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:derby:;databaseName=/home/hadoop/hive/metastore_db;create=true</value>
        <description> JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connectio n, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. </description>
    </property>
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
        <description>location of default database for the warehouse </description>
    </property>
    <property>
        <name>hive.metastore.uris</name>
        <value/>
        <description>Thrift URI for the remote metastore. Used by metastore cli ent to connect to remote metastore. </description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.apache.derby.jdbc.EmbeddedDriver</value>
        <description>Driver class name for a JDBC metastore</description>
    </property>
    <property>
        <name>javax.jdo.PersistenceManagerFactoryClass</name>
        <value>org.datanucleus.api.jdo.JDOPersistenceManagerFactory</value>
        <description>class implementing the jdo persistence</description>
    </property>
    <property>
        <name>hive.server2.thrift.min.worker.threads</name>
        <value>3</value>
    </property>
    <property>
        <name>hive.server2.thrift.max.worker.threads</name>
        <value>5</value>
    </property>
    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
    </property>
    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>localhost</value>
    </property>
    
</configuration>
stop-all.sh
start-dfs.sh
start-yarn.sh
hive/bin/schematool -initSchema -dbType derby
hive
show databases;
exit
[optional]
hive/bin/hiveserver2
wget https://dbeaver.io/files/dbeaver-ce_latest_amd64.deb
[optional]
wget wget http://ftp-stud.hs-esslingen.de/pub/Mirrors/ftp.apache.org/dist/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz
tar -xzf spark-2.3.2-bin-hadoop2.7.tgz
mv spark-2.3.2-bin-hadoop2.7/ spark
cp spark/conf/spark-env.sh.template spark/conf/spark-env.sh
nano spark/conf/spark-env.sh
export SPARK_MASTER_IP=localhost
export SPARK_WORKER_CORES=1
export SPARK_WORKER_MEMORY=800m
export SPARK_WORKER_INSTANCES=1
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_LOCAL_IP="localhost"
cp spark/conf/slaves.template spark/conf/slaves
nano spark/conf/slaves
# A Spark Worker will be started on each of the machines listed below.
localhost
stop-all.sh
start-dfs.sh
start-yarn.sh
spark-shell --master yarn
val dummy_data = 1 to 100
val dummy_RDD = sc.parallelize(dummy_data)
dummy_RDD.filter(_ < 10).collect()
exit
spark-shell
val dummy_data = 1 to 100
val dummy_RDD = sc.parallelize(dummy_data)
dummy_RDD.filter(_ < 10).collect()
exit
sudo apt-get install python
sudo apt-get install python-pip
pip2 install jupyter
jupyter notebook




